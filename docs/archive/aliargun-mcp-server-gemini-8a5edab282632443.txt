Directory structure:
‚îî‚îÄ‚îÄ aliargun-mcp-server-gemini/
    ‚îú‚îÄ‚îÄ README.md
    ‚îú‚îÄ‚îÄ CHANGELOG.md
    ‚îú‚îÄ‚îÄ CONTRIBUTING.md
    ‚îú‚îÄ‚îÄ ENHANCED_FEATURES.md
    ‚îú‚îÄ‚îÄ IMPLEMENTATION_SUMMARY.md
    ‚îú‚îÄ‚îÄ MIGRATION.md
    ‚îú‚îÄ‚îÄ PARAMETERS_REFERENCE.md
    ‚îú‚îÄ‚îÄ QUICK_REFERENCE.md
    ‚îú‚îÄ‚îÄ USAGE_GUIDE.md
    ‚îî‚îÄ‚îÄ docs/
        ‚îú‚îÄ‚îÄ claude-desktop-setup.md
        ‚îú‚îÄ‚îÄ development-guide.md
        ‚îú‚îÄ‚îÄ examples.md
        ‚îú‚îÄ‚îÄ implementation-notes.md
        ‚îî‚îÄ‚îÄ troubleshooting.md

================================================
FILE: README.md
================================================
# Gemini MCP Server

[![smithery badge](https://smithery.ai/badge/mcp-server-gemini)](https://smithery.ai/server/mcp-server-gemini)
[![npm version](https://img.shields.io/npm/v/mcp-server-gemini)](https://www.npmjs.com/package/mcp-server-gemini)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![TypeScript](https://img.shields.io/badge/TypeScript-5.3-blue)](https://www.typescriptlang.org/)
[![MCP Version](https://img.shields.io/badge/MCP-2024--11--05-green)](https://modelcontextprotocol.io/)

A powerful MCP (Model Context Protocol) server that brings Google's latest Gemini AI models to your favorite development environment. Access Gemini 2.5's thinking capabilities, vision analysis, embeddings, and more through a seamless integration.

üöÄ **Works with**: Claude Desktop, Cursor, Windsurf, and any MCP-compatible client  
üéØ **Why use this**: Get Gemini's cutting-edge AI features directly in your IDE with full parameter control  
üìö **Self-documenting**: Built-in help system means you never need to leave your editor

## Features

- **6 Powerful Tools**: Text generation, image analysis, token counting, model listing, embeddings, and self-documenting help
- **Latest Gemini Models**: Support for Gemini 2.5 series with thinking capabilities
- **Advanced Features**: JSON mode, Google Search grounding, system instructions, conversation memory
- **Full MCP Protocol**: Standard stdio communication for seamless integration with any MCP client
- **Self-Documenting**: Built-in help system - no external docs needed
- **TypeScript & ESM**: Modern, type-safe implementation

### Supported Models

| Model | Context | Features | Best For |
|-------|---------|----------|----------|
| gemini-2.5-pro | 2M tokens | Thinking, JSON, Grounding | Complex reasoning |
| gemini-2.5-flash ‚≠ê | 1M tokens | Thinking, JSON, Grounding | General use |
| gemini-2.5-flash-lite | 1M tokens | Thinking, JSON | Fast responses |
| gemini-2.0-flash | 1M tokens | JSON, Grounding | Standard tasks |
| gemini-1.5-pro | 2M tokens | JSON | Legacy support |

## Quick Start

1. **Get Gemini API Key**
   - Visit [Google AI Studio](https://makersuite.google.com/app/apikey)
   - Create a new API key
   - **IMPORTANT**: Keep your API key secure and never commit it to version control

2. **Configure Your MCP Client**

   <details>
   <summary><b>Claude Desktop</b></summary>
   
   Config location:
   - Mac: `~/Library/Application Support/Claude/claude_desktop_config.json`
   - Windows: `%APPDATA%\Claude\claude_desktop_config.json`
   - Linux: `~/.config/Claude/claude_desktop_config.json`

   ```json
   {
     "mcpServers": {
       "gemini": {
         "type": "stdio",
         "command": "npx",
         "args": ["-y", "github:aliargun/mcp-server-gemini"],
         "env": {
           "GEMINI_API_KEY": "your_api_key_here"
         }
       }
     }
   }
   ```
   </details>

   <details>
   <summary><b>Cursor</b></summary>
   
   Add to Cursor's MCP settings:
   ```json
   {
     "gemini": {
       "type": "stdio",
       "command": "npx",
       "args": ["-y", "github:aliargun/mcp-server-gemini"],
       "env": {
         "GEMINI_API_KEY": "your_api_key_here"
       }
     }
   }
   ```
   </details>

   <details>
   <summary><b>Windsurf</b></summary>
   
   Configure in Windsurf's MCP settings following their documentation.
   </details>

   <details>
   <summary><b>Other MCP Clients</b></summary>
   
   Use the standard MCP stdio configuration:
   ```json
   {
     "type": "stdio",
     "command": "npx",
     "args": ["-y", "github:aliargun/mcp-server-gemini"],
     "env": {
       "GEMINI_API_KEY": "your_api_key_here"
     }
   }
   ```
   </details>

3. **Restart Your MCP Client**

## How to Use

Once configured, you can use natural language in your MCP client to access Gemini's capabilities:

### Basic Commands
```
"Use Gemini to explain quantum computing"
"Analyze this image with Gemini" 
"List all Gemini models"
"Get help on using Gemini"
```

### Advanced Examples
```
"Use Gemini 2.5 Pro with temperature 0.3 to review this code"
"Use Gemini in JSON mode to extract key points with schema {title, summary, tags}"
"Use Gemini with grounding to research the latest in quantum computing"
```

üìñ **[See the complete Usage Guide](USAGE_GUIDE.md)** for detailed examples and advanced features.

## Why Gemini MCP Server?

- **Access Latest Models**: Use Gemini 2.5 with thinking capabilities - Google's most advanced models
- **Full Feature Set**: All Gemini API features including JSON mode, grounding, and system instructions  
- **Easy Setup**: One-line npx installation, no complex configuration needed
- **Production Ready**: Comprehensive error handling, TypeScript types, and extensive documentation
- **Active Development**: Regular updates with new Gemini features as they're released

## Documentation

- **[Usage Guide](USAGE_GUIDE.md)** - Complete guide on using all tools and features
- **[Parameters Reference](PARAMETERS_REFERENCE.md)** - Detailed documentation of all parameters
- **[Quick Reference](QUICK_REFERENCE.md)** - Quick commands cheat sheet
- **[Enhanced Features](ENHANCED_FEATURES.md)** - Detailed list of v4.0.0 capabilities
- [Claude Desktop Setup Guide](docs/claude-desktop-setup.md) - Detailed setup instructions
- [Examples and Usage](docs/examples.md) - Usage examples and advanced configuration
- [Implementation Notes](docs/implementation-notes.md) - Technical implementation details
- [Development Guide](docs/development-guide.md) - Guide for developers
- [Troubleshooting Guide](docs/troubleshooting.md) - Common issues and solutions

## Local Development

```bash
# Clone repository
git clone https://github.com/aliargun/mcp-server-gemini.git
cd mcp-server-gemini

# Install dependencies
npm install

# Set up environment variables
cp .env.example .env
# Edit .env and add your GEMINI_API_KEY

# Start development server
npm run dev
```

## Contributing

Contributions are welcome! Please see our [Contributing Guide](CONTRIBUTING.md).

## Common Issues

1. **Connection Issues**
   - Ensure your MCP client is properly restarted
   - Check the client's logs (e.g., `~/Library/Logs/Claude/mcp-server-gemini.log` for Claude Desktop on Mac)
   - Verify internet connection
   - See [Troubleshooting Guide](docs/troubleshooting.md)

2. **API Key Problems**
   - Verify API key is correct
   - Check API key has proper permissions
   - Ensure the key is set in the environment variable
   - See [Setup Guide](docs/claude-desktop-setup.md)

## Security

- API keys are handled via environment variables only
- Never commit API keys to version control
- The `.claude/` directory is excluded from git
- No sensitive data is logged or stored
- Regular security updates
- If your API key is exposed, regenerate it immediately in Google Cloud Console

## License

MIT



================================================
FILE: CHANGELOG.md
================================================
# Changelog

All notable changes to this project will be documented in this file.

The format is based on [Keep a Changelog](https://keepachangelog.com/en/1.0.0/),
and this project adheres to [Semantic Versioning](https://semver.org/spec/v2.0.0.html).

## [4.2.2] - 2025-07-08

### Fixed
- Fixed image truncation issue by adding crlfDelay: Infinity to readline interface
- Added proper UTF-8 encoding for stdin to handle large Base64 data
- Improved error handling and debugging for image analysis
- Added logging for Base64 data size to help diagnose issues

### Changed
- Enhanced error messages for image analysis failures
- Added input validation for image analysis parameters

## [4.2.1] - 2025-07-08

### Fixed
- Fixed conversation context role validation error by adding 'user' role to all user messages
- Added proper role field to generate_text, analyze_image, and count_tokens API calls
- Resolved "Invalid role" error when using conversation_id feature

## [4.2.0] - 2025-07-08

### Changed
- Cleaned up repository by removing legacy WebSocket implementation files
- Removed unused dependencies (ws and @types/ws)
- Streamlined codebase to only include stdio-based MCP implementation

### Security
- Performed comprehensive security audit
- Verified no exposed secrets or API keys
- Confirmed all dependencies are vulnerability-free
- API keys only accessed via environment variables

### Removed
- `src/server.ts` - Legacy WebSocket server
- `src/index.ts` - Old entry point
- `src/handlers.ts` - Unused handler functions
- `src/protocol.ts` - Legacy protocol definitions
- `src/stdio-server.ts` - Superseded by enhanced-stdio-server
- WebSocket dependencies

## [4.1.0] - 2025-07-07

### Added
- Self-documenting `get_help` tool for discovering features within MCP clients
- New MCP resources for documentation access:
  - `gemini://help/usage` - Usage guide
  - `gemini://help/parameters` - Parameters reference  
  - `gemini://help/examples` - Example usage patterns
- Support for `resources/read` to access documentation programmatically
- Updated documentation to support all MCP clients (not just Claude Desktop)

### Changed
- Updated README to include setup instructions for multiple MCP clients
- Enhanced documentation with comprehensive parameter references

## [4.0.0] - 2025-07-07

### Added
- Support for all latest Gemini models (as of July 2025):
  - Gemini 2.5 series with thinking capabilities
  - Gemini 2.0 series including pro-experimental
  - Legacy 1.5 models for compatibility
- 5 powerful tools:
  - `generate_text` - Advanced text generation with all features
  - `analyze_image` - Vision analysis capabilities
  - `count_tokens` - Token counting for cost estimation
  - `list_models` - Model discovery with filtering
  - `embed_text` - Text embeddings for semantic search
- Advanced features:
  - JSON mode with schema validation
  - Google Search grounding for current information
  - System instructions for behavior control
  - Conversation memory with session IDs
  - Customizable safety settings
  - Temperature, topK, topP controls
- 3 MCP prompts for common tasks
- 2 MCP resources for model and capability information

### Changed
- Complete rewrite to use stdio-based MCP protocol
- Switched to newline-delimited JSON (not Content-Length headers)
- Updated to new @google/genai SDK

## [3.0.0] - 2025-07-07

### Changed
- Migrated from WebSocket to stdio-based communication
- Fixed MCP protocol implementation based on official spec

## [2.0.0] - 2025-07-07

### Changed
- Updated from deprecated @google/generative-ai to @google/genai SDK
- Fixed TypeScript ESM configuration
- Added proper .js extensions to imports

## [1.0.0] - 2024-12-15

### Added
- Initial release with WebSocket-based MCP server
- Basic Gemini text generation support
- TypeScript implementation


================================================
FILE: CONTRIBUTING.md
================================================
# Contributing to Gemini MCP Server

Thank you for your interest in contributing to the Gemini MCP Server! This document provides guidelines for contributing to the project.

## Code of Conduct

This project follows a standard code of conduct. Please be respectful and constructive in all interactions.

## How to Contribute

### Reporting Issues

1. Check if the issue already exists in the [issue tracker](https://github.com/aliargun/mcp-server-gemini/issues)
2. If not, create a new issue with:
   - Clear description of the problem
   - Steps to reproduce
   - Expected behavior
   - Actual behavior
   - Your environment (OS, MCP client, Node.js version)
   - Relevant logs or error messages

### Suggesting Enhancements

1. Check if the enhancement has already been suggested
2. Create a new issue with the `enhancement` label
3. Describe the feature and why it would be useful
4. Provide examples of how it would work

### Pull Requests

1. Fork the repository
2. Create a new branch: `git checkout -b feature/your-feature-name`
3. Make your changes
4. Write or update tests if applicable
5. Update documentation if needed
6. Commit your changes with clear commit messages
7. Push to your fork
8. Create a pull request

#### Pull Request Guidelines

- Keep PRs focused - one feature or fix per PR
- Follow the existing code style
- Update the README.md if you're adding new features
- Add tests for new functionality
- Make sure all tests pass: `npm test`
- Update type definitions if changing APIs

## Development Setup

```bash
# Clone your fork
git clone https://github.com/YOUR_USERNAME/mcp-server-gemini.git
cd mcp-server-gemini

# Install dependencies
npm install

# Run in development mode
npm run dev

# Run tests
npm test

# Build the project
npm run build

# Lint the code
npm run lint
```

## Code Style

- TypeScript with strict mode
- ESM modules
- Use async/await over callbacks
- Add JSDoc comments for public APIs
- Follow the existing patterns in the codebase

## Testing

- Write tests for new features
- Ensure existing tests pass
- Test with multiple MCP clients if possible
- Test error cases and edge conditions

## Documentation

- Update README.md for new features
- Add JSDoc comments for new functions
- Update USAGE_GUIDE.md if adding new tools
- Update PARAMETERS_REFERENCE.md for new parameters

## Release Process

Maintainers will:
1. Review and merge PRs
2. Update version in package.json
3. Update CHANGELOG.md
4. Create a new release on GitHub
5. Publish to npm if applicable

## Questions?

Feel free to open an issue for any questions about contributing!


================================================
FILE: ENHANCED_FEATURES.md
================================================
# Enhanced Gemini MCP Server Features (v4.0.0)

This enhanced version of the Gemini MCP server includes all the latest features from Google's Gemini API as of July 2025.

## Available Models

### Gemini 2.5 Series (Latest - With Thinking Capabilities)
- **gemini-2.5-pro** - Most capable thinking model with 2M token context
- **gemini-2.5-flash** - Fast thinking model with best price/performance (1M tokens)
- **gemini-2.5-flash-lite** - Ultra-fast, cost-efficient thinking model

### Gemini 2.0 Series
- **gemini-2.0-flash** - Fast, efficient model with 1M context window
- **gemini-2.0-flash-lite** - Most cost-efficient model
- **gemini-2.0-pro-experimental** - Experimental model with 2M context, excellent for coding

### Legacy Models
- **gemini-1.5-pro** - Previous generation pro model (2M tokens)
- **gemini-1.5-flash** - Previous generation fast model (1M tokens)

## Available Tools

### 1. `generate_text` - Advanced Text Generation
Generate text with all the latest Gemini features:
- **Model Selection**: Choose any available Gemini model
- **System Instructions**: Guide model behavior with system prompts
- **Temperature Control**: Fine-tune creativity (0-2)
- **Advanced Sampling**: Control with topK and topP parameters
- **JSON Mode**: Get structured JSON output with optional schema validation
- **Google Search Grounding**: Get up-to-date information from the web
- **Safety Settings**: Configure content filtering per category
- **Conversation Memory**: Maintain context across multiple turns

### 2. `analyze_image` - Vision Analysis
Analyze images using Gemini's vision capabilities:
- Support for image URLs or base64-encoded images
- Compatible with all vision-capable models
- Natural language understanding of visual content

### 3. `count_tokens` - Token Counting
Count tokens for any text with a specific model:
- Accurate token counting for cost estimation
- Model-specific tokenization

### 4. `list_models` - Model Discovery
List all available models with filtering:
- Filter by capabilities (thinking, vision, grounding, json_mode)
- View model descriptions and context windows
- Check feature availability

### 5. `embed_text` - Text Embeddings
Generate embeddings for semantic search and similarity:
- Latest embedding models (text-embedding-004)
- Multilingual support
- High-dimensional vectors for accuracy

## Available Resources

- **gemini://models** - Detailed list of all available models
- **gemini://capabilities** - Comprehensive API capabilities documentation

## Available Prompts

### 1. `code_review` - Comprehensive Code Review
Use Gemini 2.5 Pro's thinking capabilities for in-depth code analysis

### 2. `explain_with_thinking` - Deep Explanations
Leverage thinking models for thorough explanations of complex topics

### 3. `creative_writing` - Creative Content Generation
Generate creative content with style and length control

## Advanced Features

### Thinking Models
The Gemini 2.5 series includes "thinking" capabilities that allow models to reason through problems step-by-step before responding, resulting in more accurate and thoughtful outputs.

### JSON Mode with Schema Validation
When `jsonMode` is enabled, you can provide a JSON schema to ensure the output matches your exact requirements.

### Google Search Grounding
Enable real-time web search to ground responses in current information, perfect for:
- Current events
- Technical documentation
- Fact-checking
- Up-to-date information

### Multi-turn Conversations
Maintain conversation context using `conversationId` to build more coherent, contextual interactions.

### Safety Configuration
Fine-tune safety settings per request with granular control over:
- Harassment
- Hate speech
- Sexually explicit content
- Dangerous content

## Example Usage

### Advanced Text Generation with All Features
```json
{
  "tool": "generate_text",
  "arguments": {
    "prompt": "Explain quantum computing",
    "model": "gemini-2.5-pro",
    "systemInstruction": "You are a physics professor explaining to undergraduate students",
    "temperature": 0.8,
    "maxTokens": 2048,
    "jsonMode": true,
    "jsonSchema": {
      "type": "object",
      "properties": {
        "explanation": { "type": "string" },
        "key_concepts": { "type": "array", "items": { "type": "string" } },
        "difficulty_level": { "type": "number", "minimum": 1, "maximum": 10 }
      }
    },
    "grounding": true,
    "conversationId": "quantum-discussion-001"
  }
}
```

### Image Analysis
```json
{
  "tool": "analyze_image",
  "arguments": {
    "prompt": "What's happening in this image? Describe any text, objects, and activities.",
    "imageBase64": "data:image/jpeg;base64,/9j/4AAQSkZJRg...",
    "model": "gemini-2.5-flash"
  }
}
```

## Best Practices

1. **Model Selection**:
   - Use `gemini-2.5-flash` for most tasks (best balance)
   - Use `gemini-2.5-pro` for complex reasoning and coding
   - Use `gemini-2.5-flash-lite` for high-volume, simple tasks

2. **Thinking Models**:
   - Enable for tasks requiring deep reasoning
   - Expect slightly longer response times but better quality

3. **Context Management**:
   - Use conversation IDs for multi-turn interactions
   - Monitor token usage with the count_tokens tool

4. **Safety and Grounding**:
   - Enable grounding for current information needs
   - Adjust safety settings based on your use case


================================================
FILE: IMPLEMENTATION_SUMMARY.md
================================================
# MCP Server Implementation Summary

## Overview

This document summarizes the complete implementation of the Model Context Protocol (MCP) server for Google Gemini.

## Key Implementation Details

### 1. Protocol Research

After initial attempts using WebSocket and Content-Length headers (similar to LSP), research revealed that MCP uses:
- **Newline-delimited JSON** for stdio transport
- Standard JSON-RPC 2.0 format
- No Content-Length headers required

### 2. Architecture

The server is implemented as a stdio-based Node.js application:
- Reads JSON-RPC messages from stdin (line by line)
- Writes responses to stdout (newline-delimited)
- Logs all debug information to stderr

### 3. Implemented Methods

- `initialize` - Returns server capabilities
- `tools/list` - Lists the `generate_text` tool
- `tools/call` - Executes Gemini text generation
- `resources/list` - Returns empty array (for future expansion)
- `prompts/list` - Returns empty array (for future expansion)

### 4. Key Features

- Uses the new `@google/genai` SDK (migrated from deprecated SDK)
- Full TypeScript with ESM modules
- Proper error handling and validation
- Compatible with Claude Desktop via npx

## Testing Results

The server successfully:
1. Connects to Claude Desktop
2. Responds to all protocol messages
3. Generates text using Gemini when requested
4. Handles errors gracefully

## Lessons Learned

1. **Research First**: The initial WebSocket implementation was completely wrong. Proper research of MCP documentation was crucial.
2. **Protocol Matters**: Using Content-Length headers (LSP-style) broke the communication. MCP uses simpler newline-delimited JSON.
3. **Notifications**: MCP notifications (like `notifications/initialized`) don't require responses.
4. **Debug Carefully**: All console output must go to stderr to avoid breaking the stdio protocol.

## Future Improvements

- Add more Gemini models
- Implement streaming responses
- Add resource providers for context
- Create custom prompts


================================================
FILE: MIGRATION.md
================================================
# Migration Guide - v1.x to v2.0

This guide helps you migrate from mcp-server-gemini v1.x to v2.0, which includes breaking changes required to support the latest Google Gemini SDK and modern TypeScript/ESM standards.

## Breaking Changes

### 1. Google SDK Update
The project now uses the new `@google/genai` SDK instead of the deprecated `@google/generative-ai`.

**Before:**
```javascript
import { GoogleGenerativeAI } from '@google/generative-ai';
const genAI = new GoogleGenerativeAI(apiKey);
const model = genAI.getGenerativeModel({ model: 'gemini-pro' });
```

**After:**
```javascript
import { GoogleGenAI } from '@google/genai';
const genAI = new GoogleGenAI({ apiKey });
// Use genAI.models.generateContent() directly
```

### 2. ESM Module System
The project now uses full ESM with `.js` extensions in imports.

**Before:**
```typescript
import { MCPHandlers } from './handlers';
```

**After:**
```typescript
import { MCPHandlers } from './handlers.js';
```

### 3. Type System Changes
All types are now consolidated in `src/types.ts`. The duplicate type files have been removed.

**Before:**
- Types scattered across `src/types.ts`, `src/types/index.ts`, `src/types/protocols.ts`

**After:**
- All types in `src/types.ts`

### 4. API Method Changes

#### Generate Content
**Before:**
```javascript
const result = await model.generateContent(prompt, {
  temperature: 0.7,
  maxOutputTokens: 1000
});
```

**After:**
```javascript
const result = await genAI.models.generateContent({
  model: 'gemini-2.0-flash-002',
  contents: prompt,
  config: {
    temperature: 0.7,
    maxOutputTokens: 1000
  }
});
```

#### Streaming
**Before:**
```javascript
const stream = await model.generateContentStream(prompt, options);
```

**After:**
```javascript
const stream = await genAI.models.generateContentStream({
  model: 'gemini-2.0-flash-002',
  contents: prompt,
  config: options
});
```

### 5. Model Names
The default model has changed from `gemini-pro` to `gemini-2.0-flash-002`.

## Configuration Changes

### TypeScript Configuration
The `tsconfig.json` now targets ES2022 and includes source maps:

```json
{
  "compilerOptions": {
    "target": "ES2022",
    "module": "NodeNext",
    "moduleResolution": "NodeNext",
    "sourceMap": true
    // ... other options
  }
}
```

### Package.json
Ensure your `package.json` includes:
```json
{
  "type": "module",
  "dependencies": {
    "@google/genai": "^1.8.0",
    "ws": "^8.16.0"
  }
}
```

## Migration Steps

1. **Update Dependencies**
   ```bash
   npm uninstall @google/generative-ai
   npm install @google/genai@^1.8.0
   ```

2. **Update Imports**
   - Add `.js` extensions to all relative imports
   - Update Google SDK imports to use `GoogleGenAI`

3. **Update API Calls**
   - Change `generateContent` calls to use the new object-based API
   - Update streaming calls to use `generateContentStream` with new syntax

4. **Update Model References**
   - Change `gemini-pro` to `gemini-2.0-flash-002` or another supported model

5. **Rebuild and Test**
   ```bash
   npm run build
   npm test
   ```

## Backward Compatibility

To maintain some backward compatibility:

1. The MCP protocol interface remains unchanged
2. WebSocket connection handling is the same
3. Request/response formats are preserved

## Need Help?

If you encounter issues during migration:

1. Check the [GitHub Issues](https://github.com/aliargun/mcp-server-gemini/issues)
2. Review the updated examples in the README
3. Ensure all dependencies are properly installed

## Future Deprecations

- The old `@google/generative-ai` SDK support ends September 30, 2025
- Consider updating to newer Gemini models as they become available


================================================
FILE: PARAMETERS_REFERENCE.md
================================================
# Gemini MCP Server - Complete Parameters Reference

This guide provides detailed information on all available parameters for each tool.

## 1. generate_text Tool

### Required Parameters
- **prompt** (string): The text prompt to send to Gemini

### Optional Parameters

| Parameter | Type | Default | Description | Example Values |
|-----------|------|---------|-------------|----------------|
| **model** | string | gemini-2.5-flash | Gemini model to use | gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flash |
| **systemInstruction** | string | none | System prompt to guide behavior | "You are a helpful Python tutor" |
| **temperature** | number | 0.7 | Creativity level (0-2) | 0.1 (precise), 0.7 (balanced), 1.5 (creative) |
| **maxTokens** | number | 2048 | Maximum output tokens | 100, 500, 1000, 4096 |
| **topK** | number | 40 | Top-k sampling | 1 (greedy), 40 (default), 100 (diverse) |
| **topP** | number | 0.95 | Nucleus sampling | 0.1 (focused), 0.95 (default), 1.0 (all) |
| **jsonMode** | boolean | false | Enable JSON output | true, false |
| **jsonSchema** | object | none | JSON schema validation | See examples below |
| **grounding** | boolean | false | Enable Google Search | true, false |
| **conversationId** | string | none | Maintain conversation | "chat-001", "session-123" |
| **safetySettings** | array | default | Content filtering | See safety section |

### Examples with Parameters

#### Basic Text Generation
```
"Use Gemini to explain machine learning"
```

#### With Specific Model and Temperature
```
"Use Gemini 2.5 Pro with temperature 0.2 to write technical documentation for this API"
```

#### With System Instruction
```
"Use Gemini with system instruction 'You are an expert Python developer' to review this code"
```

#### JSON Mode with Schema
```
"Use Gemini in JSON mode to analyze this text and return:
{
  type: object,
  properties: {
    sentiment: { type: string, enum: ['positive', 'negative', 'neutral'] },
    confidence: { type: number, minimum: 0, maximum: 1 },
    keywords: { type: array, items: { type: string } }
  }
}"
```

#### With Grounding
```
"Use Gemini with grounding enabled to tell me about the latest AI developments"
```

#### With Conversation Memory
```
"Start a conversation with ID 'python-help' and ask Gemini about decorators"
"Continue conversation 'python-help' and ask about generators"
```

#### With Safety Settings
```
"Use Gemini with safety settings [
  { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_NONE' }
] to analyze this medical text"
```

## 2. analyze_image Tool

### Required Parameters (one of these)
- **imageUrl** (string): URL of the image to analyze
- **imageBase64** (string): Base64-encoded image data

### Required Parameters
- **prompt** (string): Question or instruction about the image

### Optional Parameters
| Parameter | Type | Default | Description | Example Values |
|-----------|------|---------|-------------|----------------|
| **model** | string | gemini-2.5-flash | Vision-capable model | gemini-2.5-pro, gemini-2.5-flash, gemini-2.0-flash |

### Examples

#### With Image URL
```
"Analyze this image and describe what you see: https://example.com/image.jpg"
```

#### With Base64 Image
```
"What's in this screenshot? [paste image directly]"
```

#### With Specific Model
```
"Use Gemini 2.5 Pro to analyze this architecture diagram and explain the components"
```

## 3. count_tokens Tool

### Required Parameters
- **text** (string): Text to count tokens for

### Optional Parameters
| Parameter | Type | Default | Description | Example Values |
|-----------|------|---------|-------------|----------------|
| **model** | string | gemini-2.5-flash | Model for token counting | Any Gemini model |

### Examples
```
"Count tokens for this text: [your long text]"
"How many tokens would this use with gemini-2.5-pro: [text]"
```

## 4. list_models Tool

### Optional Parameters
| Parameter | Type | Default | Description | Example Values |
|-----------|------|---------|-------------|----------------|
| **filter** | string | all | Filter by capability | all, thinking, vision, grounding, json_mode |

### Examples
```
"List all Gemini models"
"Show me models with thinking capability"
"Which models support grounding?"
"List models that have JSON mode"
```

## 5. embed_text Tool

### Required Parameters
- **text** (string): Text to generate embeddings for

### Optional Parameters
| Parameter | Type | Default | Description | Example Values |
|-----------|------|---------|-------------|----------------|
| **model** | string | text-embedding-004 | Embedding model | text-embedding-004, text-multilingual-embedding-002 |

### Examples
```
"Generate embeddings for: Machine learning is fascinating"
"Create multilingual embeddings for this text using text-multilingual-embedding-002"
```

## Advanced Parameter Combinations

### Complex Analysis with All Features
```
"Use Gemini 2.5 Pro to analyze this code with:
- System instruction: 'You are a security expert'
- Temperature: 0.3
- Max tokens: 4096
- JSON mode enabled
- Schema: { security_score: number, vulnerabilities: array, recommendations: array }
- Grounding enabled for latest security practices"
```

### Creative Writing with Parameters
```
"Use Gemini 2.5 Flash to write a story with:
- Temperature: 1.5
- Max tokens: 2000
- Top-k: 100
- Top-p: 0.98
- System instruction: 'You are a creative sci-fi writer'"
```

### Conversation with Context
```
"Start conversation 'code-review-001' with Gemini 2.5 Pro using:
- System instruction: 'You are a thorough code reviewer'
- Temperature: 0.5
- Review this Python function"

"Continue conversation 'code-review-001' and ask about performance optimizations"
```

## Safety Settings Reference

### Categories
- HARM_CATEGORY_HARASSMENT
- HARM_CATEGORY_HATE_SPEECH
- HARM_CATEGORY_SEXUALLY_EXPLICIT
- HARM_CATEGORY_DANGEROUS_CONTENT

### Thresholds
- BLOCK_NONE - No blocking
- BLOCK_ONLY_HIGH - Block only high probability
- BLOCK_MEDIUM_AND_ABOVE - Block medium and high
- BLOCK_LOW_AND_ABOVE - Block all but negligible

### Example
```
"Use Gemini with safety settings:
[
  { category: 'HARM_CATEGORY_HARASSMENT', threshold: 'BLOCK_ONLY_HIGH' },
  { category: 'HARM_CATEGORY_DANGEROUS_CONTENT', threshold: 'BLOCK_NONE' }
]
to analyze this medical research paper"
```

## Model-Specific Features

### Thinking Models (2.5 series)
- gemini-2.5-pro
- gemini-2.5-flash
- gemini-2.5-flash-lite

These models have enhanced reasoning capabilities. Use them for:
- Complex problem solving
- Code analysis and generation
- Deep explanations
- Multi-step reasoning

### Models with Grounding
- gemini-2.5-pro
- gemini-2.5-flash
- gemini-2.0-flash
- gemini-2.0-pro-experimental

These can access Google Search for current information.

### All Models Support
- JSON mode
- System instructions
- Function calling
- Safety settings
- Temperature control

## Tips for Parameter Usage

1. **Start Simple**: Begin with just the prompt, add parameters as needed
2. **Model Selection**: Use 2.5-flash for most tasks, 2.5-pro for complex reasoning
3. **Temperature**: Lower for factual tasks, higher for creative tasks
4. **Token Limits**: Count tokens first for long inputs
5. **JSON Mode**: Always provide a schema for consistent output
6. **Grounding**: Enable only when you need current information
7. **Conversations**: Use IDs to maintain context across multiple queries

## Common Parameter Patterns

### For Code Review
```
model: "gemini-2.5-pro"
temperature: 0.3
systemInstruction: "You are an expert code reviewer"
jsonMode: true
maxTokens: 4096
```

### For Creative Writing
```
model: "gemini-2.5-flash"
temperature: 1.2
topK: 100
topP: 0.98
maxTokens: 2000
```

### For Factual Analysis
```
model: "gemini-2.5-flash"
temperature: 0.1
grounding: true
jsonMode: true
```

### For Learning/Tutoring
```
model: "gemini-2.5-flash"
systemInstruction: "You are a patient teacher"
temperature: 0.7
conversationId: "learning-session-001"
```


================================================
FILE: QUICK_REFERENCE.md
================================================
# Gemini MCP Server - Quick Reference

## üöÄ Quick Commands

### Text Generation
```
"Use Gemini to [your prompt]"
"Ask Gemini 2.5 Pro to [complex task]"
"Use Gemini with temperature 1.5 for [creative task]"
```

### Image Analysis
```
"Analyze this image with Gemini: [image]"
"What's in this screenshot?"
"Describe this diagram using Gemini"
```

### Token Counting
```
"Count tokens for: [text]"
"How many tokens in this prompt?"
```

### Model Info
```
"List all Gemini models"
"Show thinking models"
"Which models support grounding?"
```

### Embeddings
```
"Generate embeddings for: [text]"
"Create semantic vectors for search"
```

## üéØ Model Selection

| Task | Recommended Model | Why |
|------|------------------|-----|
| Complex reasoning | gemini-2.5-pro | Thinking capability |
| General use | gemini-2.5-flash | Best balance |
| Fast responses | gemini-2.5-flash-lite | Ultra-fast |
| Cost-sensitive | gemini-2.0-flash-lite | Most economical |
| Coding | gemini-2.0-pro-experimental | Code-optimized |

## ‚ö° Advanced Features

### JSON Output
```
"Use Gemini in JSON mode to analyze sentiment and return {sentiment, confidence, keywords}"
```

### Google Search Grounding
```
"Use Gemini with grounding to research [current topic]"
```

### System Instructions
```
"Use Gemini as a Python tutor to explain [concept]"
```

### Conversation Memory
```
"Start conversation 'chat-001' with Gemini about [topic]"
"Continue chat-001 and ask about [related topic]"
```

## üé® Temperature Guide

- **0.1-0.3**: Precise, factual (documentation, analysis)
- **0.5-0.8**: Balanced (default: 0.7)
- **1.0-1.5**: Creative (stories, brainstorming)
- **1.5-2.0**: Very creative (poetry, fiction)

## üí° Pro Tips

1. **Specify models** for better control
2. **Use grounding** for current information
3. **Enable JSON mode** for structured data
4. **Set temperature** based on task type
5. **Use conversation IDs** for context
6. **Count tokens** before long operations

## üîß Troubleshooting

- **Tools not showing?** Restart Claude Desktop
- **Errors?** Check logs at `~/Library/Logs/Claude/`
- **API issues?** Verify your API key
- **Need help?** See [Usage Guide](USAGE_GUIDE.md)


================================================
FILE: USAGE_GUIDE.md
================================================
# Gemini MCP Server Usage Guide

This guide explains how to use all the available tools in the Gemini MCP Server through Claude Desktop.

## Available Tools Overview

Once the Gemini MCP Server is configured in Claude Desktop, you can access these 6 powerful tools:

1. **generate_text** - Advanced text generation with all Gemini features
2. **analyze_image** - Vision analysis for images
3. **count_tokens** - Token counting for cost estimation
4. **list_models** - List available Gemini models
5. **embed_text** - Generate text embeddings
6. **get_help** - Get help and usage information directly in Claude

üìã **[See Complete Parameters Reference](PARAMETERS_REFERENCE.md)** for detailed parameter documentation with all available options and examples.

## How to Use Tools in Claude Desktop

When the MCP server is properly configured, you can simply ask Claude to use these tools naturally in conversation. Here are examples:

### 1. Text Generation Tool (`generate_text`)

**Basic Usage:**
```
"Use Gemini to explain quantum computing"
"Ask Gemini 2.5 Pro to write a Python function for sorting"
"Have Gemini generate a creative story about space exploration"
```

**Advanced Usage with Parameters:**
```
"Use Gemini 2.5 Pro with high temperature (1.5) to write a creative poem"
"Ask Gemini to explain machine learning in JSON format with key concepts"
"Use Gemini with Google Search grounding to tell me about today's news"
```

**Example with All Features:**
```
"Use Gemini 2.5 Pro to analyze this code and return a JSON response with:
- code_quality (1-10 scale)
- issues (array of problems)
- suggestions (array of improvements)
Enable grounding for latest best practices."
```

### 2. Image Analysis Tool (`analyze_image`)

**Usage:**
```
"Use Gemini to analyze this image: [paste image or provide URL]"
"What's in this screenshot? [attach image]"
"Describe the architecture diagram in this image using Gemini"
```

**Note:** You can either:
- Paste an image directly into Claude
- Provide an image URL
- Attach an image file

### 3. Token Counting Tool (`count_tokens`)

**Usage:**
```
"Count tokens for this text using Gemini: [your text]"
"How many tokens would this prompt use with gemini-2.5-flash?"
"Check token count for my document with Gemini 2.5 Pro"
```

### 4. Model Listing Tool (`list_models`)

**Usage:**
```
"List all available Gemini models"
"Show me Gemini models that support thinking"
"Which Gemini models have grounding capability?"
"List models with JSON mode support"
```

**Filter Options:**
- `all` - Show all models
- `thinking` - Models with thinking capabilities
- `vision` - Vision-capable models
- `grounding` - Models with Google Search grounding
- `json_mode` - Models supporting JSON output

### 5. Text Embedding Tool (`embed_text`)

**Usage:**
```
"Generate embeddings for this text: [your text]"
"Create semantic embeddings using Gemini for similarity search"
"Get embeddings for these product descriptions using text-embedding-004"
```

### 6. Help Tool (`get_help`)

**Usage:**
```
"Get help on using Gemini MCP server"
"Get help on tools"
"Get help on models"
"Get help on parameters"
"Get help on examples"
"Show me a quick start guide"
```

**Available Topics:**
- `overview` - General introduction and features
- `tools` - Detailed information about each tool
- `models` - Available models and selection guide
- `parameters` - Complete parameter reference
- `examples` - Usage examples for common tasks
- `quick-start` - Quick start guide

This tool is perfect for learning how to use the MCP server without leaving Claude Desktop!

## Advanced Features

### System Instructions
You can guide the model's behavior:
```
"Use Gemini with system instruction 'You are a helpful Python tutor' to explain decorators"
```

### JSON Mode with Schema
Get structured output:
```
"Use Gemini in JSON mode to analyze this text and return:
{
  sentiment: 'positive' | 'negative' | 'neutral',
  confidence: number (0-1),
  key_phrases: string[]
}"
```

### Conversation Memory
Maintain context across multiple requests:
```
"Start a conversation with Gemini about machine learning (use conversation ID: ml-chat-001)"
"Continue the ml-chat-001 conversation and ask about neural networks"
```

### Temperature Control
Adjust creativity:
- Low (0.1-0.3): Focused, deterministic responses
- Medium (0.5-0.8): Balanced responses (default: 0.7)
- High (1.0-2.0): Creative, diverse responses

```
"Use Gemini with temperature 0.2 for precise technical documentation"
"Use Gemini with temperature 1.5 for creative writing"
```

### Safety Settings
Configure content filtering:
```
"Use Gemini with relaxed safety settings for medical content analysis"
```

## Model Selection Guide

### For Complex Reasoning & Coding
- **gemini-2.5-pro** - Best for complex problems, deep analysis
- **gemini-2.0-pro-experimental** - Excellent for coding tasks

### For Fast Responses
- **gemini-2.5-flash** (recommended) - Best balance of speed and capability
- **gemini-2.5-flash-lite** - Ultra-fast for simple tasks

### For Cost Efficiency
- **gemini-2.0-flash-lite** - Most cost-effective
- **gemini-1.5-flash** - Good for basic tasks

## Tips for Best Results

1. **Be Specific**: Instead of "use Gemini", specify the model and parameters you need
2. **Use Thinking Models**: For complex problems, explicitly ask for gemini-2.5-pro or gemini-2.5-flash
3. **Enable Grounding**: For current information, ask to "enable grounding" or "use Google Search"
4. **Structure Output**: Use JSON mode for structured data extraction
5. **Monitor Tokens**: Use count_tokens before expensive operations

## Example Conversations

### Code Review
```
You: "Use Gemini 2.5 Pro to review this code for security issues, performance problems, and suggest improvements"
```

### Research with Grounding
```
You: "Use Gemini with grounding enabled to research the latest developments in quantum computing"
```

### Creative Writing
```
You: "Use Gemini 2.5 Flash with temperature 1.2 to write a short sci-fi story about AI consciousness"
```

### Multi-turn Conversation
```
You: "Start a conversation with Gemini about web development (ID: webdev-001)"
You: "Continue webdev-001 and ask about React best practices"
You: "Continue webdev-001 and ask about state management options"
```

## Troubleshooting

If tools aren't working:
1. Check MCP server status in Claude Desktop
2. Verify your API key is set correctly
3. Look for errors in the logs
4. Restart Claude Desktop
5. Ensure you're using the latest version (v4.0.0)

## Rate Limits

Be aware of Google's API rate limits:
- Requests per minute vary by model
- Token limits differ between models
- Monitor usage to avoid hitting limits

For detailed API limits, check [Google AI Studio](https://makersuite.google.com/app/apikey).


================================================
FILE: docs/claude-desktop-setup.md
================================================
# Claude Desktop MCP Configuration Guide

## Overview

This guide explains how to configure Claude Desktop to use the Gemini MCP server. The Model Context Protocol (MCP) allows Claude to interact with external AI models and tools.

## Configuration Steps

### 1. Get Gemini API Key

1. Visit [Google AI Studio](https://makersuite.google.com/app/apikey)
2. Create or sign in to your Google account
3. Generate a new API key
4. Copy the API key for later use

### 2. Locate Configuration File

The configuration file location depends on your operating system:

- **macOS**:
  ```
  ~/Library/Application Support/Claude/claude_desktop_config.json
  ```

- **Windows**:
  ```
  %APPDATA%\Claude\claude_desktop_config.json
  ```

- **Linux**:
  ```
  ~/.config/Claude/claude_desktop_config.json
  ```

### 3. Edit Configuration

1. Open the configuration file in a text editor
2. Add or update the mcpServers section:

```json
{
  "mcpServers": {
    "gemini": {
      "command": "npx",
      "args": ["-y", "github:aliargun/mcp-server-gemini"],
      "env": {
        "GEMINI_API_KEY": "your_api_key_here"
      }
    }
  }
}
```

### 4. Verify Setup

1. Save the configuration file
2. Restart Claude Desktop completely
3. Test the connection by asking Claude:
   "Can you verify if the Gemini MCP connection is working?"

## Advanced Configuration

### Debug Mode
```json
{
  "mcpServers": {
    "gemini": {
      "command": "npx",
      "args": ["-y", "github:aliargun/mcp-server-gemini"],
      "env": {
        "GEMINI_API_KEY": "your_api_key_here",
        "DEBUG": "true"
      }
    }
  }
}
```

### Custom Port
```json
{
  "mcpServers": {
    "gemini": {
      "command": "npx",
      "args": ["-y", "github:aliargun/mcp-server-gemini"],
      "env": {
        "GEMINI_API_KEY": "your_api_key_here",
        "PORT": "3006"
      }
    }
  }
}
```

## Troubleshooting

### Common Issues

1. **Configuration File Not Found**
   - Run Claude Desktop at least once
   - Create the directory if it doesn't exist
   - Create an empty JSON file if needed

2. **Connection Errors**
   - Check if the port is available
   - Verify internet connection
   - Check firewall settings

3. **API Key Issues**
   - Verify the key is correct
   - Ensure no whitespace in the key
   - Check API key permissions

### Error Messages

1. **"Cannot connect to MCP server"**
   - Check if the server is running
   - Verify port settings
   - Check network connectivity

2. **"Invalid API key"**
   - Verify API key in config
   - Regenerate API key if needed
   - Check for copying errors

## Security Notes

1. **API Key Storage**
   - Keep your API key secure
   - Don't share the configuration file
   - Regularly rotate API keys

2. **File Permissions**
   - Set appropriate file permissions
   - Restrict access to config file
   - Use environment variables when possible

## Additional Resources

1. [Gemini API Documentation](https://ai.google.dev/docs)
2. [Claude Desktop Documentation](https://www.anthropic.com/claude)
3. [MCP Protocol Specification](https://modelcontextprotocol.io)



================================================
FILE: docs/development-guide.md
================================================
# Development Guide

## Environment Setup

1. Prerequisites
   - Node.js 18+
   - npm/yarn
   - TypeScript
   - Gemini API key

2. Installation
```bash
git clone https://github.com/aliargun/mcp-server-gemini.git
cd mcp-server-gemini
npm install
```

3. Configuration
```bash
# Set your Gemini API key
export GEMINI_API_KEY=your_api_key_here
```

## Development Workflow

1. Start Development Server
```bash
npm run dev
```

2. Build for Production
```bash
npm run build
```

3. Run Tests
```bash
npm test
```

## Project Structure

```
src/
‚îú‚îÄ‚îÄ index.ts         # Main entry point
‚îú‚îÄ‚îÄ types/           # TypeScript type definitions
‚îú‚îÄ‚îÄ utils/           # Utility functions
test/                # Test files
docs/                # Documentation
```

## Adding Features

1. Create new message handler:
```typescript
if (request.method === 'newMethod') {
  // Handle new method
}
```

2. Add capability:
```typescript
capabilities: {
  experimental: {
    newFeature: true
  }
}
```

## Testing

1. Unit Tests
```typescript
describe('Message Handler', () => {
  it('handles new method', () => {
    // Test implementation
  });
});
```

2. Integration Tests
```typescript
describe('WebSocket Server', () => {
  it('connects and processes messages', () => {
    // Test implementation
  });
});
```

## Debugging

1. Enable Debug Logging
```typescript
const DEBUG = true;
if (DEBUG) console.log('Debug:', message);
```

2. Use WebSocket Client
```bash
wscat -c ws://localhost:3005
```

## Best Practices

1. Code Style
   - Use TypeScript
   - Follow existing patterns
   - Document public APIs

2. Error Handling
   - Use type-safe errors
   - Provide meaningful messages
   - Log appropriately

3. Testing
   - Write unit tests
   - Add integration tests
   - Test error cases



================================================
FILE: docs/examples.md
================================================
# Examples and Usage Instructions

## Claude Desktop Setup

1. **Locate the Configuration File**
   ```
   Mac: ~/Library/Application Support/Claude/claude_desktop_config.json
   Windows: %APPDATA%\Claude\claude_desktop_config.json
   Linux: ~/.config/Claude/claude_desktop_config.json
   ```

2. **Add Gemini MCP Configuration**
   ```json
   {
     "mcpServers": {
       "gemini": {
         "command": "npx",
         "args": ["-y", "github:aliargun/mcp-server-gemini"],
         "env": {
           "GEMINI_API_KEY": "your_api_key_here"
         }
       }
     }
   }
   ```

3. **Restart Claude Desktop**
   - Close Claude Desktop completely
   - Relaunch the application
   - The Gemini provider should now be available

## Example Interactions

### Basic Usage
```
Claude: I can now access Gemini through the MCP connection. Would you like me to compare our responses to a question?

Human: Yes, please compare how we respond to: "Explain quantum computing in simple terms."

Claude: I'll use both my own knowledge and ask Gemini through the MCP connection...
```

### Advanced Features

1. **Parameter Control**
   ```json
   {
     "method": "generate",
     "params": {
       "prompt": "Your prompt here",
       "temperature": 0.7,
       "maxTokens": 1000
     }
   }
   ```

2. **Streaming Responses**
   ```json
   {
     "method": "generate",
     "params": {
       "prompt": "Your prompt here",
       "stream": true
     }
   }
   ```

## Troubleshooting Common Setup Issues

1. **Config File Not Found**
   - Make sure Claude Desktop has been run at least once
   - Check the path for your operating system
   - Create the file if it doesn't exist

2. **API Key Issues**
   - Get your API key from Google AI Studio
   - Ensure the key has proper permissions
   - Check for any whitespace in the key

3. **Connection Issues**
   - Verify Claude Desktop is running
   - Check if port 3005 is available
   - Look for any firewall restrictions

## Best Practices

1. **API Key Security**
   - Never share your API key
   - Use environment variables when possible
   - Rotate keys periodically

2. **Resource Management**
   - Monitor API usage
   - Implement rate limiting
   - Handle long responses appropriately

## Advanced Configuration

### Custom Port Configuration
```json
{
  "mcpServers": {
    "gemini": {
      "command": "npx",
      "args": ["-y", "github:aliargun/mcp-server-gemini"],
      "env": {
        "GEMINI_API_KEY": "your_api_key_here",
        "PORT": "3006"
      }
    }
  }
}
```

### Development Setup
```json
{
  "mcpServers": {
    "gemini": {
      "command": "node",
      "args": ["/path/to/local/mcp-server-gemini/dist/index.js"],
      "env": {
        "GEMINI_API_KEY": "your_api_key_here",
        "DEBUG": "true"
      }
    }
  }
}
```

## Using with Other MCP Servers

### Multiple Providers Example
```json
{
  "mcpServers": {
    "gemini": {
      "command": "npx",
      "args": ["-y", "github:aliargun/mcp-server-gemini"],
      "env": {
        "GEMINI_API_KEY": "your_gemini_key"
      }
    },
    "openai": {
      "command": "npx",
      "args": ["-y", "@mzxrai/mcp-openai@latest"],
      "env": {
        "OPENAI_API_KEY": "your_openai_key"
      }
    }
  }
}
```

## Verification Steps

1. Check Server Status
```bash
curl -I http://localhost:3005
```

2. Test WebSocket Connection
```bash
wscat -c ws://localhost:3005
```

3. Verify MCP Integration
```
Ask Claude: "Can you verify if the Gemini MCP connection is working?"
```



================================================
FILE: docs/implementation-notes.md
================================================
# Implementation Notes

## Overview

This MCP server implements the Model Context Protocol for Google's Gemini API. It provides a standardized way for Claude Desktop to interact with Gemini models.

## Protocol Implementation

### Initialization Flow
```typescript
// Client sends initialize request
{
  "jsonrpc": "2.0",
  "id": 1,
  "method": "initialize"
}

// Server responds with capabilities
{
  "jsonrpc": "2.0",
  "id": 1,
  "result": {
    "protocolVersion": "2024-11-05",
    "capabilities": {...}
  }
}
```

### Content Generation
```typescript
// Client sends generation request
{
  "jsonrpc": "2.0",
  "id": 2,
  "method": "generate",
  "params": {
    "prompt": "Hello, world!"
  }
}

// Server responds with generated content
{
  "jsonrpc": "2.0",
  "id": 2,
  "result": {
    "type": "completion",
    "content": "Generated text..."
  }
}
```

## Key Components

1. WebSocket Server
   - Handles client connections
   - Manages message routing
   - Implements protocol lifecycle

2. Gemini Integration
   - Model initialization
   - Content generation
   - Error handling

3. Message Processing
   - JSON-RPC parsing
   - Protocol validation
   - Response formatting

## Security Considerations

1. API Key Handling
   - Environment variables only
   - No logging of sensitive data
   - Secure key rotation support

2. Input Validation
   - Request format validation
   - Parameter sanitization
   - Error boundary handling

## Performance

1. Connection Management
   - Single WebSocket connection
   - Efficient message routing
   - Resource cleanup

2. Error Handling
   - Graceful error recovery
   - Detailed error messages
   - Proper status codes



================================================
FILE: docs/troubleshooting.md
================================================
# Troubleshooting Guide

## Common Issues

### Connection Problems

1. Port Already in Use
```bash
Error: EADDRINUSE: address already in use :::3005
```
Solution:
- Check if another process is using port 3005
- Kill the existing process
- Change the port number

2. WebSocket Connection Failed
```
Error: Connection refused
```
Solution:
- Verify server is running
- Check firewall settings
- Confirm correct port

### API Issues

1. Invalid API Key
```
Error: Invalid API key provided
```
Solution:
- Check GEMINI_API_KEY environment variable
- Verify API key is valid
- Regenerate API key if needed

2. Rate Limiting
```
Error: Resource exhausted
```
Solution:
- Implement backoff strategy
- Check quota limits
- Upgrade API tier if needed

## Protocol Errors

1. Invalid Message Format
```json
Error: Parse error (-32700)
```
Solution:
- Check JSON syntax
- Verify message format
- Validate against schema

2. Method Not Found
```json
Error: Method not found (-32601)
```
Solution:
- Check method name
- Verify protocol version
- Update capabilities

## Debugging Steps

1. Enable Debug Mode
```bash
DEBUG=true npm start
```

2. Check Logs
```bash
tail -f debug.log
```

3. Monitor WebSocket Traffic
```bash
wscat -c ws://localhost:3005
```

## Getting Help

1. Check Documentation
- Review implementation notes
- Check protocol specification
- Read troubleshooting guide

2. Open Issues
- Search existing issues
- Provide error details
- Include reproduction steps

3. Community Support
- Join discussions
- Ask questions
- Share solutions


